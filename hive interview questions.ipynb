{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3c266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. What is the definition of Hive? What is the present version of Hive?\n",
    "\n",
    " hive is a datawarehouse tool to process structured data on top of hadoop.\n",
    "Hive is a data warehouse system which is used to analyze structured data. \n",
    "It is built on the top of Hadoop. It was developed by Facebook.\n",
    "Hive provides the functionality of reading, writing, \n",
    "and managing large datasets residing in distributed storage.\n",
    "\n",
    "3.1 is the present version of hive.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5485a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "2. Is Hive suitable to be used for OLTP systems? Why?\n",
    "\n",
    "hive not suitable for OLTP systems because it dosnot support row level data insertion.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7eeb8b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. How is HIVE different from RDBMS? \n",
    "Does hive support ACID transactions.\n",
    "If not then give the proper reason.\n",
    "\n",
    "hive is used to maintain datawarehouse where rdbms is used to maintain database.\n",
    "hive uses hql where rdbms uses sql .\n",
    "normalized and denormalized data are stored in hive .only normalized type is stored in rdbms .\n",
    "hive supports partitioning where rdbms doesnot.\n",
    "\n",
    "hive supports ACID transactions on tables that stores orc file formate.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17013f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Explain the hive architecture and the different components of a Hive\n",
    "architecture?\n",
    "hive has 4 sections in its architecture \n",
    "1.hive client\n",
    " it has thrift app,jdbc app,odbc app\n",
    "    \n",
    "2.hive services \n",
    " it has hive server , hive driver ,compiler,optimizer.\n",
    "3.processing and resource mangement\n",
    " it has mapreduce, yarn\n",
    "4.distributed management i.e, hdfs\n",
    "\n",
    "#thrift client \n",
    "hive server is based on thrift and that is why it can serve the thrift clients.\n",
    "\n",
    "#jdbc clients \n",
    "hive allows for java applications to connect it on jdbc driver.\n",
    "\n",
    "#odbc clients\n",
    "hive allows odbc based applicationsalso.\n",
    "\n",
    "#hive driver\n",
    "\n",
    "it receives hql statements from users and creates session handles for query and sends the query to \n",
    "compiler.\n",
    "\n",
    "#hive compiler \n",
    "it type checks the query blocks based on meta data stored in meta store and creates an execution plan.\n",
    "the execution plan is DAG\n",
    "\n",
    "#optimizer\n",
    "\n",
    "it performs the transformation on execution plan and split the task to improve efficiency.\n",
    "\n",
    "#meta store \n",
    "it is the central repository  which store the metadata i.e,structure of  tables,partiotions ,columns.\n",
    "\n",
    "the defalult meta store is derby DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b42a097",
   "metadata": {},
   "outputs": [],
   "source": [
    "5. Mention what Hive query processor does?\n",
    "And Mention what are the\n",
    "components of a Hive query processor?\n",
    "\n",
    "hive query processor run sql like queries called hql which internally \n",
    "converted to map reduce jobs.\n",
    "\n",
    "\n",
    "components:\n",
    "    \n",
    "parse and semantic analysis ,meta data layer\n",
    "type interface,sessions,map/reduce execution engine\n",
    "plan components,hive function framework etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946a3835",
   "metadata": {},
   "outputs": [],
   "source": [
    "6. What are the three different modes in which\n",
    "we can operate Hive?\n",
    "\n",
    "local mode,distributed mode,pseudo distributed mode.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11290fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "7. Features and Limitations of Hive.\n",
    "#limitations :\n",
    "it does not support OLTP  \n",
    "it does not support subqueries\n",
    "it has high latency\n",
    "Hive table doesnot  support delete or update operations.\n",
    "\n",
    "#features\n",
    "scalable,flexible,\n",
    "designed for querying and managing structured \n",
    "data stored in tables,\n",
    "schema is stored in meta data and processed data stored in hdfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2ed311",
   "metadata": {},
   "outputs": [],
   "source": [
    "8. How to create a Database in HIVE\n",
    "\n",
    "\n",
    "create database database_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77775333",
   "metadata": {},
   "outputs": [],
   "source": [
    "9. How to create a table in HIVE?\n",
    "\n",
    "create database database_name;\n",
    "use database_name;\n",
    "create table table_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0b56a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "10.What do you mean by describe and describe extended and describe\n",
    "formatted with respect to database and table\n",
    "\n",
    "\n",
    "DESCRIBE DATABASE statement returns the metadata of an existing database\n",
    "\n",
    "describe extended - This will show table columns, data types, and other details of the table. Other details will be displayed in single line. describe formatted - This will show table columns, data types, and other details of the table. Other details will be displayed into multiple lines.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdb81e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "11.How to skip header rows from a table in Hive?\n",
    "\n",
    "tblproperties(\"skip.head.line.count\" =\"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2130df1",
   "metadata": {},
   "outputs": [],
   "source": [
    "12.What is a hive operator? What are the different types of hive operators?\n",
    "\n",
    "Apache Hive provides various Built-in operators for data operations to be implemented on the tables present inside Apache Hive warehouse. Hive operators are used for mathematical operations on operands.\n",
    "\n",
    "\n",
    "There are four types of operators in Hive:\n",
    "\n",
    "Relational Operators\n",
    "Arithmetic Operators\n",
    "Logical Operators\n",
    "Complex Operators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83fe5aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "13.Explain about the Hive Built-In Functions\n",
    "\n",
    "The Hive Built-in functions are categorized as Mathematical function, Collection function, Type conversion function, Date function, Conditional function, and String function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4684365",
   "metadata": {},
   "outputs": [],
   "source": [
    "14. Write hive DDL and DML commands.\n",
    "\n",
    "The several types of Hive DDL commands are:\n",
    "CREATE.\n",
    "SHOW.\n",
    "DESCRIBE.\n",
    "USE.\n",
    "DROP.\n",
    "ALTER.\n",
    "TRUNCATE.\n",
    "\n",
    "The various Hive DML commands are:\n",
    "LOAD.\n",
    "SELECT.\n",
    "INSERT.\n",
    "DELETE.\n",
    "UPDATE.\n",
    "EXPORT.\n",
    "IMPORT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5235698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "15.Explain about SORT BY, ORDER BY, DISTRIBUTE BY and\n",
    "CLUSTER BY in Hive.\n",
    "\n",
    "sort by is used to sort data either in ascending order or in decending order by using asc ,desc commands\n",
    "order by is used to retrieve the details based on one column and sort the result set by ascending or decending order\n",
    "hive uses distributed by to distibute the rows among the reducers\n",
    "cluster by is use as an alternative  for both distributed\n",
    "by and sort by cluses in hql it distributed the rows among redusers. cluster by clumns will go to multiple reducers.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6d9f2db",
   "metadata": {},
   "outputs": [],
   "source": [
    "16.Difference between \"Internal Table\" and \"External Table\" and Mention\n",
    "when to choose “Internal Table” and “External Table” in Hive?\n",
    "\n",
    "Managed tables are Hive owned tables where the entire lifecycle of the tables' data are managed and controlled by Hive. External tables are tables \n",
    "where Hive has loose coupling with the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85621809",
   "metadata": {},
   "outputs": [],
   "source": [
    "17.Where does the data of a Hive table get stored?\n",
    "\n",
    "hive table is stored in metastore.which is a database by default it is derby bd ,in cloudera vm it is mysql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81d8a5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "18.Is it possible to change the default location of a managed table?\n",
    " yes it is possible to change the location of managed table \n",
    "using  command location <hdfspath>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "19.What is a metastore in Hive? What is the default database provided by\n",
    "Apache Hive for metastore?\n",
    "\n",
    "hive table is stored in metastore.which is a database by default it is derby bd ,in cloudera vm it is mysql. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43527e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "20.Why does Hive not store metadata information in HDFS?\n",
    "\n",
    "The reason for choosing RDBMS is to achieve low latency as HDFS read/write operations are time consuming processes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8074fed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "21.What is a partition in Hive? And Why do we perform partitioning in\n",
    "Hive?\n",
    "\n",
    "The partitioning in Hive means dividing the table into some parts based on the values of a particular column like date, course, city or country. The advantage of partitioning is that since the data is stored in slices, the query response time becomes faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f67b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "22.What is the difference between dynamic partitioning and static\n",
    "partitioning?\n",
    "\n",
    "In static or manual partitioning, it is required to pass the values of partitioned columns manually while loading the data into the table. Hence, the data file doesn't contain the partitioned columns\n",
    "\n",
    "In dynamic partitioning, the values of partitioned columns exist within the table. So, it is not required to pass the values of partitioned columns manually.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3383cf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "23.How do you check if a particular partition exists?\n",
    "\n",
    "SHOW PARTITIONS table_name\n",
    "PARTION(partitioned_column='partition_value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b423729",
   "metadata": {},
   "outputs": [],
   "source": [
    "24.How can you stop a partition form being queried?\n",
    "\n",
    "by using ENABLE OFFLNE clause with ALTER TABLE statement \n",
    "\n",
    "ALTER TABLE table_name \n",
    "PARTITION (PARTITION_SPEC) \n",
    "ENABLE OFFLINE;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbfb4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "25.Why do we need buckets? How Hive distributes the rows into buckets?\n",
    "\n",
    "In bucketing, the partitions can be subdivided into buckets based on the hash function of a column. \n",
    "It gives extra structure to the data which can be used for more efficient queries.\n",
    "\n",
    "The concept of bucketing is based on the hashing technique.\n",
    "Here, modules of current column value and the number of required buckets is calculated \n",
    "(let say, F(x) % 3).\n",
    "Now, based on the resulted value, the data is stored into the corresponding bucket.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd8fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "26.In Hive, how can you enable buckets?\n",
    "\n",
    "set hive.enforce.bucketing=true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2317153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "27.How does bucketing help in the faster execution of queries?\n",
    "\n",
    " it uses hash function to partion data in buckets so hive  first from the given query find hash function then then check that particular bucket which maches the hash key so there is no need to analyze the entair data for query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caf4b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "28.How to optimise Hive Performance? Explain in very detail.\n",
    "\n",
    "Partitioning Tables: Hive partitioning is an effective method to improve the query performance on larger tables. ...\n",
    "De-normalizing data,\n",
    "Compress map/reduce output,\n",
    "Map join,\n",
    "Input Format Selection\n",
    "Parallel execution\n",
    "Vectorization\n",
    "Unit Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da074d8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "29. What is the use of Hcatalog?\n",
    "\n",
    "\n",
    "Basically, to share data structures with external systems we use Hcatalog. \n",
    "It offers access to hive metastore to users of other tools on Hadoop. \n",
    "Hence, they can read and write data to hive’s data warehouse.\n",
    "\n",
    "HCatalog is a tool that allows you to access Hive metastore tables within Pig, Spark SQL, and/or custom MapReduce applications.\n",
    "HCatalog has a REST interface and command line client that allows you to create tables or do other operations.\n",
    "You then write your applications to access the tables using HCatalog libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9558eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "30. Explain about the different types of join in Hive.\n",
    "\n",
    "There are  4 different types of joins in HiveQL –\n",
    "\n",
    "JOIN-  It is very similar to Outer Join in SQL\n",
    "\n",
    "FULL OUTER JOIN – This join Combines the records of both the left and right outer tables. \n",
    "                   Basically, that fulfill the join condition.\n",
    "\n",
    "LEFT OUTER JOIN- Through this Join, All the rows from the left table are returned even if there are no matches in the right table.\n",
    "\n",
    "RIGHT OUTER JOIN – Here also, all the rows from the right table are returned even if there are no matches in the left table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1bd90",
   "metadata": {},
   "outputs": [],
   "source": [
    "31.Is it possible to create a Cartesian join between 2 tables, using Hive?\n",
    "\n",
    "The SQL CROSS JOIN produces a result set which is the number of rows in the first table multiplied by the number of rows in the second table if no WHERE clause is used along with CROSS JOIN. \n",
    "This kind of result is called as Cartesian Product.\n",
    "If WHERE clause is used with CROSS JOIN, it functions like an INNER JOIN.\n",
    "\n",
    "Cross join, also known as Cartesian product, is a way of joining multiple tables \n",
    "in which all the rows or tuples from one table are paired with the rows and tuples from another table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a9e0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "32.Explain the SMB Join in Hive?\n",
    "\n",
    "SMB is a join performed on bucket tables that have the same sorted, bucket, and join condition columns. It reads data from both bucket tables and performs common joins (map and reduce triggered) on the bucket tables. \n",
    "We need to enable the following properties to use SMB.\n",
    "\n",
    "> SET hive.input.format=> org.apache.hadoop.hive.ql.io.BucketizedHiveInputFormat;\n",
    "> SET hive.auto.convert.sortmerge.join=true;\n",
    "> SET hive.optimize.bucketmapjoin=true;\n",
    "> SET hive.optimize.bucketmapjoin.sortedmerge=true;\n",
    "> SET hive.auto.convert.sortmerge.join.noconditionaltask=true;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec33a585",
   "metadata": {},
   "outputs": [],
   "source": [
    "33.What is the difference between order by and sort by which one we should\n",
    "use?\n",
    "\n",
    "The difference between \"order by\" and \"sort by\" is that the former guarantees total order in the output while the latter only guarantees ordering of the rows within a reducer. \n",
    "If there are more than one reducer, \"sort by\" may give partially ordered final results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de830b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "34.What is the usefulness of the DISTRIBUTED BY clause in Hive?\n",
    "\n",
    "DISTRIBUTE BY clause is used to distribute the input rows among reducers.\n",
    "It ensures that all rows for the same key columns are going to the same reducer. \n",
    "So, if we need to partition the data on some key column,\n",
    "we can use the DISTRIBUTE BY clause in the hive queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5d561f",
   "metadata": {},
   "outputs": [],
   "source": [
    "35.How does data transfer happen from HDFS to Hive?\n",
    "\n",
    "Hive does not store data. Hive is Sql like interface on top of HDFS and used for data processing. Hive creates MapReduce Jobs from SQL.\n",
    "\n",
    "Data resides in HDFS and Hive created MapReduce jobs reads the data and stored it back in HDFS.\n",
    "\n",
    "load data inpath(\"/user/hive/warehouse/dir/data.csv\") into hive_table\n",
    "\n",
    "\n",
    "Basically, the user need not LOAD DATA that moves the files to the /user/hive/warehouse/.\n",
    "But only if data is already present in HDFS.\n",
    "Hence, using the keyword external that creates the table definition in the hive metastore  the user just has to define the table.\n",
    "Create external table table_name (\n",
    " id int,\n",
    " myfields string\n",
    ")\n",
    "location ‘/my/location/in/hdfs’;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5a48d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "36.Wherever (Different Directory) I run the hive query, it creates a new\n",
    "metastore_db, please explain the reason for it?\n",
    "\n",
    "Basically, it creates the local metastore, while we run the hive in embedded mode.\n",
    "Also, it looks whether metastore already exist or not before creating the metastore.\n",
    "Hence, in configuration file hive-site.xml. Property is “javax.jdo.option.ConnectionURL” with default value “jdbc:derby:;databaseName=metastore_db;create=true” this property is defined. Hence, to change the behavior change the location to the absolute path, thus metastore will be used from that location.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18261add",
   "metadata": {},
   "outputs": [],
   "source": [
    "37.What will happen in case you have not issued the command: ‘SET\n",
    "hive.enforce.bucketing=true;’ before bucketing a table in Hive?\n",
    "\n",
    "The command: 'SET hive. enforce. bucketing=true;' allows one to have the correct number of reducer while using 'CLUSTER BY' clause for bucketing a column.\n",
    "In case it's not done, one may find the number of files that will be generated in the table directory to be not equal to the number of buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8c1a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "38.Can a table be renamed in Hive?\n",
    "\n",
    "we can rename the table name in the hive.\n",
    "we need to use the alter command.\n",
    "This command allows you to change the table name as shown below.\n",
    " ALTER TABLE old_name RENAME new_name;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a988518",
   "metadata": {},
   "outputs": [],
   "source": [
    "39.Write a query to insert a new column(new_col INT) into a hive table at a\n",
    "position before an existing column (x_col)\n",
    "\n",
    "ALTER TABLE hive_table\n",
    "CHANGE COLUMN new_col INT\n",
    "BEFORE x_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a585e29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "40.What is serde operation in HIVE?\n",
    "\n",
    "\n",
    "SerDe means Serializer and Deserializer. Hive uses SerDe and FileFormat to read and write table rows.\n",
    "Main use of SerDe interface is for IO operations.\n",
    "A SerDe allows hive to read the data from the table and write it back to the HDFS in any custom format. If we have unstructured data, then we use RegEx SerDe which will instruct hive how to handle that record.\n",
    "We can also write our own Custom SerDe in any format.\n",
    "\n",
    "\n",
    "\n",
    "Deserializer is conversion of string or binary data into java object when we any submit any query.\n",
    "\n",
    "Serializer converts java object into string or binary object. It is used when writing the data such as insert- select statements.Hive currently uses these FileFormat classes to read and write HDFS files\n",
    "\n",
    "\n",
    "TextInputFormat/HiveIgnoreKeyTextOutputFormat: These 2 classes read/write data in plain text file format.\n",
    "\n",
    "SequenceFileInputFormat/SequenceFileOutputFormat: These 2 classes read/write data in Hadoop SequenceFile format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17f11b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "41.Explain how Hive Deserializes and serialises the data?\n",
    "\n",
    "Deserializer is conversion of string or binary data into java object when we any submit any query.\n",
    "\n",
    "Serializer converts java object into string or binary object. It is used when writing the data such as insert- select statements.\n",
    "Hive currently uses these FileFormat classes to read and write HDFS files\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8940bfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "42.Write the name of the built-in serde in hive.\n",
    "\n",
    "Avro (Hive 0.9.1 and later)\n",
    "ORC (Hive 0.11 and later)\n",
    "RegEx\n",
    "Thrift\n",
    "Parquet (Hive 0.13 and later)\n",
    "CSV (Hive 0.14 and later)\n",
    "JsonSerDe (Hive 0.12 and later in hcatalog-core)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f1e291",
   "metadata": {},
   "outputs": [],
   "source": [
    "43.What is the need of custom Serde?\n",
    "\n",
    "A SerDe allows Hive to read in data from a table, and write it back out to HDFS in any custom format.\n",
    "Anyone can write their own SerDe for their own data formats. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51dcfc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "44.Can you write the name of a complex data type(collection data types) in\n",
    "Hive?\n",
    "\n",
    "Array,map,struct,union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3f65935",
   "metadata": {},
   "outputs": [],
   "source": [
    "45.Can hive queries be executed from script files? How?\n",
    "\n",
    "yes we have to svae the query file with queries with .sql  extention \n",
    "and using the command \n",
    "\n",
    "hive -f /path of the file \n",
    " it is exicuted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c426d1dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "46.What are the default record and field delimiter used for hive text files?\n",
    "\n",
    "The STORED AS textfile subclause instructs Hive to create the table in Textfile (the default) format\n",
    "\n",
    "“Fields terminated by ‘,’ ” indicates that the columns in the input file are separated by the  ‘,’ delimiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e8564",
   "metadata": {},
   "outputs": [],
   "source": [
    "47.How do you list all databases in Hive whose name starts with s?\n",
    "\n",
    "show databases like s*;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5b9e89",
   "metadata": {},
   "outputs": [],
   "source": [
    "48.What is the difference between LIKE and RLIKE operators in Hive?\n",
    "\n",
    "LIKE is an operator similar to LIKE in SQL. We use LIKE to search for string with similar text.\n",
    "\n",
    "E.g. user_name LIKE ‘*Smith’\n",
    "\n",
    "\n",
    "RLIKE stands for Right Like. RLIKE is special function in hive where if any sub-string of A matches with B then it evaluates to true.\n",
    "Usually while matching a sub-string we put * symbol before or after the sub-string, but here in RLIKE , we don't need to put % symbol for a simple match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2149cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "49.How to change the column data type in Hive?\n",
    "\n",
    "ALTER TABLE CHANGE column_name column_name new_dataytpe;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca45783",
   "metadata": {},
   "outputs": [],
   "source": [
    "50.How will you convert the string ’51.2’ to a float value in the particular\n",
    "column?\n",
    "\n",
    "selcect cast('51.2',float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5855df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "51.What will be the result when you cast ‘abc’ (string) as INT?\n",
    "\n",
    "\n",
    "the cast operation will fail and returns null\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b7f083",
   "metadata": {},
   "outputs": [],
   "source": [
    "52.What does the following query do?\n",
    "a. INSERT OVERWRITE TABLE employees\n",
    "b. PARTITION (country, state)\n",
    "c. SELECT ..., se.cnty, se.st\n",
    "d. FROM staged_employees se;\n",
    "\n",
    "\n",
    "Hive determines the values of the partition keys, country and state from the last two columns \n",
    "in the SELECT clant\n",
    "\n",
    " This is why we use different names in staged-employees and also emphasize the relationship \n",
    "    between source column values and output partition values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0a436b",
   "metadata": {},
   "outputs": [],
   "source": [
    "53.Write a query where you can overwrite data in a new table from the\n",
    "existing table.\n",
    "\n",
    "CREATE TABLE order AS\n",
    "   SELECT [ column1, column2...columnN ]\n",
    "   FROM order2\n",
    "   [ WHERE ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a1e6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "54.What is the maximum size of a string data type supported by Hive?\n",
    "Explain how Hive supports binary formats.\n",
    "\n",
    "\n",
    "The maximum size of a string data type supported by Hive is 2 GB.\n",
    "Hive supports the text file format by default, and it also supports the binary format\n",
    "sequence files, ORC files, Avro data files, and Parquet files. \n",
    "Sequence file: It is a splittable, compressible, and row-oriented file with a general binary format.\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8537a",
   "metadata": {},
   "outputs": [],
   "source": [
    "55. What File Formats and Applications Does Hive Support?\n",
    "\n",
    "Hive supports the text file format by default, and it also supports the binary format\n",
    "sequence files, ORC files, Avro data files, and Parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f0641cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "56.How do ORC format tables help Hive to enhance its performance?\n",
    "\n",
    "Using the ORC format leads to a reduction in the size of the data stored, as this \n",
    "file format has high compression ratios. As the data size is reduced, \n",
    "the time to read and write the data is also reduced.\n",
    "The ORC format improves query performance also by the way it stores data in a file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de77a655",
   "metadata": {},
   "outputs": [],
   "source": [
    "57.How can Hive avoid mapreduce while processing the query?\n",
    "\n",
    "You can make Hive avoid MapReduce to return query results by \n",
    "setting the hive.exec.mode.local.auto property to ‘true’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2869571",
   "metadata": {},
   "outputs": [],
   "source": [
    "58.What is view and indexing in hive?\n",
    "\n",
    "A view allows a query to be saved and treated like a table. \n",
    "It is a logical construct, as it does not store data like a table.\n",
    "When a query references a view, the information in its definition is combined with the rest\n",
    "of the query by Hive’s query planner.\n",
    "Logically,you can imagine that Hive executes the view and then uses the results in the rest of the query.\n",
    "\n",
    "\n",
    "Indexes are a pointer or reference to a record in a table as in relational databases.\n",
    "Indexing is a relatively new feature in Hive. \n",
    "In Hive, the index table is different than the main table.\n",
    "Indexes facilitate in making query execution or search operation faster.\n",
    "However, storing indexes require disk space and creating an index involves cost.\n",
    "So, the use of indexes may not always be of any benefit. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddac607",
   "metadata": {},
   "outputs": [],
   "source": [
    "59.Can the name of a view be the same as the name of a hive table?\n",
    "\n",
    "\n",
    "The name of a view must be unique, and it cannot\n",
    "be the same as any table or database or view's name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2609558",
   "metadata": {},
   "outputs": [],
   "source": [
    "60.What types of costs are associated in creating indexes on hive tables?\n",
    "\n",
    "Basically, there is a processing cost in arranging the values of the\n",
    "column on which index is created since Indexes occupies.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6605a138",
   "metadata": {},
   "outputs": [],
   "source": [
    "61.Give the command to see the indexes on a table.\n",
    "\n",
    "show index from table_name;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab4cf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "62. Explain the process to access subdirectories recursively in Hive queries.\n",
    "\n",
    "Set mapred.input.dir.recursive=true;\n",
    "\n",
    "Set hive.mapred.supports.subdirectories=true;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba8abc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "63.If you run a select * query in Hive, why doesn't it run MapReduce?\n",
    "\n",
    "hive.fetch.task.conversion property can (FETCH task) minimize latency of mapreduce overhead.\n",
    "\n",
    "When queried SELECT, FILTER, LIMIT queries, this property skip mapreduce and using FETCH task.\n",
    "\n",
    "As a result Hive can execute query without run mapreduce task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130617a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "64.What are the uses of Hive Explode?\n",
    "\n",
    "Explode() function takes an array as an input and results elements of that array as separate rows. \n",
    "Select explode(column_name) from table_name; \n",
    "In below example, we have column technology as array of string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b078be47",
   "metadata": {},
   "outputs": [],
   "source": [
    "65. What is the available mechanism for connecting applications when we\n",
    "run Hive as a server?\n",
    "\n",
    "Thrift Client: Using Thrift, we can call Hive commands from various programming languages,\n",
    "such as C++, PHP, Java, Python, and Ruby."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7818f89a",
   "metadata": {},
   "outputs": [],
   "source": [
    "66.Can the default location of a managed table be changed in Hive?\n",
    "\n",
    "by using the LOCATION keyword, we can change the default location of Managed tables \n",
    "while creating the managed table in Hive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4153a57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "67.What is the Hive ObjectInspector function?\n",
    "\n",
    "Hive ObjectInspector is a group of flexible APIs to inspect value in different data representation,\n",
    "and developers can extend those API as needed, \n",
    "so technically, object inspector supports arbitrary data type in java.\n",
    "\n",
    "ObjectInspector helps analyze the internal structure of a row object and \n",
    "the individual structure of columns in Hive. It also provides a uniform way \n",
    "to access complex objects that can be stored in multiple formats in the memory.\n",
    "\n",
    "An instance of Java class\n",
    "A standard Java object\n",
    "A lazily initialized object\n",
    "ObjectInspector tells the structure of the object and also the ways to access \n",
    "the internal fields inside the object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e0ae0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "68.What is UDF in Hive?\n",
    "\n",
    "In Hive, the users can define own functions to meet certain client requirements. \n",
    "These are known as UDFs in Hive. User Defined Functions written in Java for specific modules.\n",
    "Some of UDFs are specifically designed for the reusability of code in application frameworks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e530216",
   "metadata": {},
   "outputs": [],
   "source": [
    "69.Write a query to extract data from hdfs to hive.\n",
    "\n",
    "load data inpath /user/dfs/file.txt into table hive_table_name; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85711dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "70.What is TextInputFormat and SequenceFileInputFormat in hive.\n",
    "\n",
    "\n",
    "This is very familiar input format in the Hadoop. The input will be given as key and value to Mapper,\n",
    "where key and value are generated in record reader. The record reader is just like a multiplexing.\n",
    "For TextInputFormat, No need to create external record reader.\n",
    "          Key generated-----LongWritable(position ,generally \"\\n\" or offset)\n",
    "          Value generated-----Text of each line.\n",
    "The important point in the TextInputFormat is\n",
    "a) Number of maps created is equal to number of files given in input path.\n",
    "b) For each line, map method in mapper will be called.\n",
    "\n",
    "\n",
    "The sequence file is the file has lot of importnace in hadoop.It has equal importance as \"AVRO file\" .\n",
    "Hadoop is the technology best suited for file with huge size,than many number of files with small size.\n",
    "So in this case,One of the possible solution to process many number of files with small size is\n",
    "\"Merging of files\".\n",
    " so we people use sequence file (Binary key/value) to merge large number of small files as file name as \n",
    "key and total content as value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d5ed81",
   "metadata": {},
   "outputs": [],
   "source": [
    "71.How can you prevent a large job from running for a long time in a hive?\n",
    "\n",
    "This can be achieved by setting the MapReduce jobs to execute in strict mode \n",
    " set hive.mapred.mode=strict;\n",
    "\n",
    "The strict mode ensures that the queries on partitioned tables cannot execute without\n",
    "defining a WHERE clause."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5483d09",
   "metadata": {},
   "outputs": [],
   "source": [
    "72.When do we use explode in Hive?\n",
    "\n",
    "The explode function explodes an array to multiple rows. Returns a row-set with a single column (col),\n",
    "one row for each element from the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e9c3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "73.Can Hive process any type of data formats? Why? Explain in very detail\n",
    "\n",
    "Hive supports four file formats those are TEXTFILE, SEQUENCEFILE, ORC and RCFILE\n",
    "(Record Columnar File). For single user metadata storage, Hive uses derby database and for multiple\n",
    "user Metadata or shared Metadata case Hive uses MYSQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03380e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "74.Whenever we run a Hive query, a new metastore_db is created. Why?\n",
    "\n",
    "A local metastore is created when we run Hive in an embedded mode.\n",
    "\n",
    "\n",
    "with the default value:\n",
    "jdbc:derby:;databaseName=metastore_db;create=true\n",
    "\n",
    "Therefore, we have to change the behavior of the location to an absolute path so that from that\n",
    "location the metastore can be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df71ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "75.Can we change the data type of a column in a hive table? Write a\n",
    "complete query.\n",
    "\n",
    "ALTER TABLE table_name CHANGE col_name col_name newType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36986250",
   "metadata": {},
   "outputs": [],
   "source": [
    "76.While loading data into a hive table using the LOAD DATA clause, how\n",
    "do you specify it is a hdfs file and not a local file ?\n",
    "  \n",
    "    \n",
    "by removing local keyword and specifing the hdfs file path in the command. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf44c1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "77.What is the precedence order in Hive configuration?\n",
    "\n",
    "\n",
    "In Hive we can use following precedence order to set the configurable properties.\n",
    "\n",
    "Hive SET command has the highest priority\n",
    "-hiveconf option from Hive Command Line\n",
    "hive-site.xml file\n",
    "hive-default.xml file\n",
    "hadoop-site.xml file\n",
    "hadoop-default.xml file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4990eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "78.Which interface is used for accessing the Hive metastore?\n",
    "\n",
    "WebHCat API web interface can be used for Hive commands. It is a REST API that allows applications\n",
    "to make HTTP requests to access the Hive metastore (HCatalog DDL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927c861",
   "metadata": {},
   "outputs": [],
   "source": [
    "79.Is it possible to compress json in the Hive external table ?\n",
    "\n",
    "As text data, JSON data compresses nicely. \n",
    "That's why gzip is our first option to reduce the JSON data size.\n",
    "\n",
    "by using gzip command we can compress the json data\n",
    "\n",
    "gzip file_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3794fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "80.What is the difference between local and remote metastores?\n",
    "\n",
    "Local Metastore:- Here metastore service still runs in the same JVM as Hive but it connects \n",
    "    to a database running in a separate process either on same machine or on a remote machine.\n",
    "Remote Metastore:- Metastore runs in its own separate JVM not on hive service JVM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b360d3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "81.What is the purpose of archiving tables in Hive?\n",
    "\n",
    "You can use Hadoop archiving to reduce the number of hdfs files in the Hive table partition.\n",
    "Hive has built in functions to convert Hive table partition into Hadoop Archive (HAR). \n",
    "HAR does not compress the files, it is analogous to the Linux tar command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d9a9d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "82.What is DBPROPERTY in Hive?\n",
    "\n",
    "By default, the Hive database will be created inside the default warehouse directory i.e\n",
    "/user/hive/warehouse. But if we want we can store the database in some other HDFS location as well\n",
    "but mentioning the same in the location field.\n",
    "\n",
    "The DB properties are nothing but mentioning the details about the database created by the user. \n",
    "Suppose the name of the user, the type of the database and the tables it has, \n",
    "the date on which the database is created etc.\n",
    "\n",
    "\n",
    "This makes the other user easy the recognize the database and use it according to the requirement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e237db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "83.Differentiate between local mode and MapReduce mode in Hive.\n",
    "\n",
    "\n",
    "\n",
    "Both MapReduce mode and local mode seem same to the user but the difference is the way they execute.\n",
    "\n",
    "MapReduce mode:\n",
    "\n",
    "In MapReduce mode, Pig script is executed on Hadoop cluster. \n",
    "The Pig scripts are converted into MapReduce jobs and then executed on Hadoop cluster (hdfs)\n",
    "\n",
    "Local mode: \n",
    "\n",
    "In this mode, Pig script runs on a Single machine without the need of Hadoop cluster or hdfs.\n",
    "Local mode is used for development purpose to see how the script would behave in an actual environment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc7c0c6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb96072",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "452466c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
